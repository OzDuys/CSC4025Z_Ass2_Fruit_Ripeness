% Report: Fruit Ripeness Classification (Assignment 2)
% This document summarises dataset, baselines, CNN, methodology, and analysis.
\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{grffile} % improves filename handling (incl. spaces)
\usepackage{float}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}
\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue, citecolor=blue}

% Safe image include: shows a placeholder box if file is missing
\newcommand{\img}[2][]{%
  \IfFileExists{#2}{\includegraphics[#1]{#2}}{\fbox{\parbox[c][3cm][c]{0.9\linewidth}{\centering Image placeholder: #2}}}%
}

% Graphics search paths (handles spaces in directory names on modern LaTeX)
\graphicspath{{./}{Dataset/}{training notebooks/cnn/}{training notebooks/Baseline Algorithms/knn/}{training notebooks/Baseline Algorithms/hog/}{results/40_epochs/}{results/figures/}}

% Readability tweaks
\setlength{\parskip}{0.6em}
\setlength{\parindent}{0pt}

\title{Fruit Ripeness Classification with Convolutional Neural Networks\\\large CSC4025Z: Assignment 2}
\author{Group: Max Elkington (ELKMAX001), Oscar Duys (DYSPIE002),\\ Catalina Thomson (ALTCAT001)}
\date{\today}
    
\begin{document}
\maketitle

\section{Problem Formulation}
\textbf{Task:} Multi-class image classification with 9 classes (fruit type \(\times\) ripeness state). Inputs are RGB images; outputs are discrete class labels. The intended application is automated fruit quality detection and sorting.

\textbf{Application:} In commercial agriculture and packing facilities, this tool can be used to enable fast quality assessment and replace manual sorting. This reduces labour costs while improving throughput. Retail and grocery stores can use this tool for inventory management, ensuring only ripe products are shelved and identifying stock requiring removal or immediate sale to minimise food waste. The technology also benefits food processing industries by automating the selection of fruits at specific ripeness stages for different products (e.g. ripe bananas for smoothies versus unripe for longer-term retail).


\textbf{Metrics:} We report accuracy for quick comparison and macro-F1 as the primary selection metric during tuning to weight classes equally. We additionally track balanced accuracy and per-class diagnostics (confusion matrices).

\textbf{Ethics:} The system targets fruit imagery for quality assessment and does not process personal or sensitive data. Risks are primarily around over-reliance on automation in quality control; human oversight is recommended.

\section{Dataset}
The dataset contains images of bananas, apples, and oranges across unripe, ripe, and rotten states, organised by class folders \footnote{See \texttt{Dataset/dataset\_description.md}.}. Images are relatively consistent, which makes colour information a strong indicator for ripeness. Training and test sets were provided in the Kaggle dataset by \texttt{leftin} (updated 4 months ago)\footnote{\href{https://www.kaggle.com/datasets/leftin/fruit-ripeness-unripe-ripe-and-rotten}{https://www.kaggle.com/datasets/leftin/fruit-ripeness-unripe-ripe-and-rotten}}. After an 85/15 split of the supplied training folder (seeded for reproducibility) we obtain 13,784 training images, 2,433 validation images, and 3,246 test images across the nine classes (Table~\ref{tab:dataset_split}).
We standardise inputs to \(224\times224\) RGB. For training, light augmentations (random flips/rotations, colour jitter, optional random erasing) are used in tuning; validation and test use deterministic resizing and normalisation only—no stochastic augmentation is applied at evaluation time.

\begin{figure}[H]
    \centering
    \img[width=0.95\linewidth]{dataset_samples.png}
    \caption{Sample Images from Kaggle dataset. Each row is a different class.}
    \label{fig:dataset-sample-images}
\end{figure}

\section{Baselines}
To evaluate the effectiveness of our CNN approach, we implemented two complementary baseline methods representing different feature extraction philosophies: k-Nearest Neighbours on raw pixels and HOG features with linear classifiers.

\subsection{k-Nearest Neighbours}

Our first baseline employs k-Nearest Neighbours (kNN), a distance-based classifier that stores all training images and classifies new images by majority vote among the k nearest neighbours in colour pixel space. We resized images to 64×64 pixels and flattened them into 12,288-dimensional vectors (64×64×3 RGB channels), testing k values from 1 to 20. Remarkably, kNN achieved 74.4\% accuracy with k=1, far exceeding typical raw pixel baseline performance of 30–50\%. This success stems from the dataset's favourable characteristics across most of the dataset: studio-quality images with consistent backgrounds, centred framing, and uniform lighting create well-separated clusters in pixel space, while ripeness states exhibit distinctive colour signatures—green for unripe, bright colours for fresh, and brown discolouration for rotten fruits.
Despite this strong performance, kNN has many practical limitations. The approach requires storing all training images (50--200 MB), suffers from slow prediction times due to exhaustive comparisons with every training sample, and provides no learned representation beyond memorised examples. Furthermore, the fact that k=1 outperformed higher values (k=20 dropped to 63.8\%) suggests that while the training set provides good coverage with minimal class overlap, the method relies heavily on nearest-neighbour memorisation rather than generalisable feature learning. These drawbacks limit practical deployment despite strong headline accuracy.

\begin{figure}[H]
  \centering
  \img[width=0.45\linewidth]{baseline_knn_k_tuning.png}
  \caption{kNN: effect of K on performance (expected trend).}
  \label{fig:knn-k}
\end{figure}

\subsection{HOG features + Linear Classifiers}
Our second baseline employs Histogram of Oriented Gradients (HOG), a  technique that extracts hand-crafted shape and texture features. HOG divides 128×128 grayscale images into small cells, computes gradient orientations at each pixel, and creates histograms of these orientations that are normalised across overlapping blocks. This produces a ~3,600-dimensional feature vector encoding edge patterns and local texture. We trained two linear classifiers on these features: Logistic Regression achieved 59.3\% accuracy, while Linear SVM reached 53.6\%.

Both HOG-based models underperformed kNN by approximately 15\% due to a critical limitation: HOG operates on grayscale images, discarding all colour information. Since fruit ripeness is fundamentally colour-dependent, removing colour eliminates the primary discriminative signal. While HOG effectively captures shape patterns like spotting and bruising, these prove insufficient without colour context. However, HOG offers practical advantages over kNN—compact models (1.3 MB), fast prediction, and interpretable features. The comparison illuminates that kNN preserves colour at the cost of no abstraction, while HOG provides structured features but sacrifices critical colour information.

\section{CNN Baseline}

\subsection{Architecture and Configuration}
To establish a performance baseline for our fruit ripeness classification task, we implemented a conventional convolutional neural network using PyTorch. (This will be referred to as \textbf{SimpleCNN} in the rest of the report.) The network comprises three convolutional blocks, with each block containing a 3×3 convolution, batch normalisation, ReLU activation, and 2×2 max pooling. The feature channels increase progressively from 32 to 64 to 128 across the three layers. A three-layer architecture was chosen as it provides sufficient depth to capture hierarchical visual features—early layers detect simple edges and textures, middle layers identify fruit-specific patterns, and deeper layers recognise complex ripeness indicators—whilst remaining computationally efficient and less prone to overfitting on our moderately-sized dataset. The convolutional blocks feed into a fully connected classifier with 256 hidden units and 50\% dropout, producing predictions for the nine classes (three fruit types × three ripeness states).

\subsection{Training Longer Unlocks Performance}

We began by exploring the effect of training for larger number of epochs (i.e. training for longer) to see the effect this would have.\\

The first model, trained for 5 epochs, achieved 72.3\% test accuracy—surpassing our non-CNN baseline and confirming that even limited training enabled effective feature extraction. Increasing the training duration to 20 epochs improved accuracy to 84.5\%, with training curves showing continued learning and no signs of convergence. Based on this, we trained a final model for 40 epochs, reaching 92.96\% test accuracy. The steady progression in accuracy (72.3\% → 84.5\% → 92.96\%) indicates that while the CNN architecture is well-suited to the task, it requires sufficient training iterations to capture the nuanced visual differences between fruit ripeness states. These results establish 40 epochs as an appropriate training duration for this baseline architecture and dataset, forming a solid reference point for subsequent model comparisons.


\begin{figure}[H]
    \centering
    \img[width=0.85\linewidth]{Baseline40.png}
    \caption{Baseline CNN training curves (40 epochs).}
    \label{fig:placeholder}
\end{figure}

The hyperparameters used in all three training runs were chosen to reflect a standard choice for such a task. The Adam optimiser with a learning rate of $1\mathrm{e}{-4}$ was selected for its robust performance on vision tasks. We used a batch size of 32, split the data into 85\% training, 15\% validation and applied data augmentation (random flips, rotations, and colour jittering) to improve generalisation. All experiments used a fixed random seed of 42 for reproducibility.


\section{Hyperparameter Tuning}
\subsection{Methodology}
We broadened our search beyond the three-layer \textbf{SimpleCNN} and adopted \textbf{macro-F1} as the primary selection metric (rather than accuracy) to weight classes equally. Hyper-parameter tuning was orchestrated in a Google Colab notebook using Weights\&Biases (W\&B) sweeps. Each run trained for 15 epochs (chosen for a balance between convergence and compute). The objective maximised \emph{validation} macro-F1. The search space spans: Architectures, Optimisers, Data, and Regularisation. The sweep ranges searched can be seen below.

\subsection{Architectures}
\textbf{SimpleCNN}\; A lightweight baseline with three convolution\,+\,pooling blocks (32/64/128 channels) and a small fully connected head. It learns colour and texture features directly without specialised blocks, trading ultimate accuracy for simplicity and speed.

\textbf{ResNet-18}\; A residual network with identity shortcuts that bypass one or more layers, enabling stable optimisation of deeper models via residual learning. The 18-layer variant uses BasicBlocks with two 3\,\(\times\)\,3 convolutions per block and a final linear classifier; we insert a dropout layer (rate from the sweep configuration) between the global average pooling output and the classifier head.\footnote{He et al., Deep Residual Learning for Image Recognition, arXiv:1512.03385}

\textbf{MobileNetV3-Small}\; An efficient architecture for mobile settings built from inverted residual bottlenecks with depthwise separable convolutions, squeeze-and-excitation (SE), and the \textit{h-swish}/\textit{h-sigmoid} activations. It balances accuracy and latency with a compact parameter footprint.\footnote{Howard et al., Searching for MobileNetV3, arXiv:1905.02244}

\textbf{EfficientNet-B0}\; A family baseline that uses MBConv (inverted residual + depthwise separable) blocks with SE; the family is scaled using compound width/depth/resolution multipliers discovered via NAS. B0 is the base configuration.\footnote{Tan and Le, EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, arXiv:1905.11946}

\subsection{Results and Selection}
We analysed \textbf{31 runs} (24 completed, i.e. ran all the way to 15 epochs and tested on the test set) across the four architectures. Model selection used validation macro-F1; we report corresponding test metrics for finished runs.

\paragraph{Top configurations (test set).}
\begin{center}
\begin{tabular}{lcccccccc}
\toprule
Arch & LR & WD & Batch & Dropout & CJ & RE & Test macro-F1 \\
\midrule
resnet18 & 3.65e-4 & 1.15e-6 & 16 & 0.444 & 0.15 & 0.00 & 0.985 \\
effnet-b0 & 4.34e-4 & 5.32e-3 & 32 & 0.164 & 0.30 & 0.25 & 0.982 \\
resnet18 & 4.92e-5 & 8.42e-6 & 16 & 0.517 & 0.15 & 0.25 & 0.960 \\
effnet-b0 & 1.71e-4 & 1.36e-4 & 32 & 0.539 & 0.15 & 0.10 & 0.960 \\
effnet-b0 & 1.21e-4 & 3.85e-6 & 16 & 0.116 & 0.30 & 0.00 & 0.952 \\
mobilenetv3-s & 3.84e-4 & 1.31e-3 & 48 & 0.178 & 0.00 & 0.00 & 0.944 \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Sweep ranges.}
\begin{center}
\begin{tabular}{lcc}
\toprule
Hyper-parameter & Range / Values & Notes \\
\midrule
Learning rate & $1\times10^{-5}$ -- $3\times10^{-3}$ & Log-uniform (AdamW/Adam/SGD) \\
Weight decay & $1\times10^{-6}$ -- $1\times10^{-2}$ & Log-uniform \\
Dropout & 0.10 -- 0.60 & Applied to classifier / head \\
Batch size & \{16, 32, 48, 64\} & Limited by GPU memory \\
Colour jitter & \{0.0, 0.15, 0.30\} & Train augmentation only \\
Random erasing & \{0.0, 0.10, 0.25\} & Applied after colour jitter \\
Rotation & \{0, 10\degree\} & Train augmentation only \\
Optimiser & \{AdamW, Adam, SGD\} & With cosine or plateau scheduler \\
\bottomrule
\end{tabular}
\end{center}

ResNet-18 is the strongest (best test macro-F1 0.985; test accuracy 0.988), closely followed by EfficientNet-B0 (best macro-F1 0.982). SimpleCNN sweeps now peak around macro-F1 $\approx 0.901$, narrowing the gap to MobileNetV3-Small (best $\approx 0.944$).

\paragraph{Architecture-level outcomes.}
\begin{itemize}
  \item \textbf{ResNet-18} (4 runs): mean test macro-F1 0.959; mean test accuracy 0.966.
  \item \textbf{EfficientNet-B0} (5 runs): mean test macro-F1 0.938; mean test accuracy 0.946.
  \item \textbf{MobileNetV3-Small} (4 runs): mean test macro-F1 0.885; mean test accuracy 0.899.
  \item \textbf{SimpleCNN} (11 runs): mean test macro-F1 0.854 ($\sigma\!\approx\!0.044$); mean test accuracy 0.872; best macro-F1 $\approx\!0.901$.
\end{itemize}

\begin{figure}[H]
  \centering
  \begin{subfigure}{0.48\linewidth}
    \centering
    \img[width=\linewidth]{val_macro_f1_ for_each_arch_in_training.png}
    \caption{Validation macro-F1}
    \label{fig:val_macro_f1}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\linewidth}
    \centering
    \img[width=\linewidth]{val_accuracy_for_each_arch_in_training.png}
    \caption{Validation accuracy}
    \label{fig:val_accuracy}
  \end{subfigure}
  \caption{Validation macro-F1 and accuracy for each architecture across epochs. Shaded error bars indicate standard error.}
  \label{fig:val_metrics_for_each_arch_in_training}
\end{figure}


The curves above are highly correlated: across architectures, validation accuracy and macro-F1 move together. This suggests relatively balanced class support and similar per-class performance; nonetheless, we use macro-F1 for model selection to guard against any latent imbalance.


\paragraph{Hyperparameter insights.}
\begin{itemize}
  \item \textbf{ResNet-18:} conservative LR ($\leq$5e-4) and minimal WD ($\leq$1e-5) work well; higher dropout (0.45--0.55) and limited random erasing add regularisation.
  \item \textbf{EfficientNet-B0:} LR 1e-4--4e-4 with colour jitter up to 0.3 and random erasing $\leq$0.25; batch 32 outperforms 16.
  \item \textbf{MobileNetV3-Small:} larger batches ($\geq$48) with little to no colour jitter; AdamW LR $\approx$3.8e-4 performed best.
  \item \textbf{SimpleCNN:} parameter importance ranks colour jitter highest (positive), with dropout trending negative beyond $\approx$0.4; moderate random erasing (0.1--0.25) helps.
\end{itemize}

\begin{figure}[H]
  \centering
  \img[width=0.8\linewidth]{param_importance_basic_cnn.png}
  \caption{Parameter importance for SimpleCNN sweep (macro-F1).}
  \label{fig:param-importance-simplecnn}
\end{figure}

\paragraph{Selection.} We select a \textbf{from-scratch ResNet-18} (run: \emph{still-sweep-1}) with LR 3.65e-4, weight decay 1.15e-6, batch 16, dropout 0.444, colour jitter 0.15, random erasing 0.00. This configuration achieved validation macro-F1 0.979 and test macro-F1 0.985 (test accuracy 0.988).

\begin{figure}[H]
  \centering
  \img[width=0.5\linewidth]{roc_sweep.png}
  \caption{Macro-averaged ROC curves across the sweep. The selected run (still-sweep-1) appears in red; other runs are shown in grey.}
  \label{fig:best-roc}
\end{figure}

\section{Experimental Setup}
\textbf{Frameworks:} PyTorch and torchvision.\newline
\textbf{Environment:} Local and Colab GPU.\newline
\textbf{Reproducibility:} Fixed seeds for data splits and training where applicable; metrics and artefacts logged per run.\newline
\textbf{Evaluation:} Final models are evaluated on the held-out test set; validation drives model selection.

\section{Results Summary}
\begin{center}
\begin{tabular}{lccc}
\toprule
Model & Test macro-F1 & Test Accuracy & Notes \\
\midrule
Random (9 classes) & 0.111& 11.1\% & Reference \\
HOG + Logistic & 0.565& 59.3\% & Grayscale features\\
HOG + Linear SVM & 0.511& 53.6\% & Grayscale features\\
kNN (raw RGB, K=1) & 0.743& 74.4\% & Strong colour signal \\
\midrule
\multicolumn{4}{l}{\textit{Deep Learning Models: 3 Layer CNN, Default Hyper-parameters}} \\[2pt]
\midrule
CNN baseline (5 epochs) & 0.704 & 72.3\% & Three-block SimpleCNN \\
CNN baseline (20 epochs) & 0.836 & 84.5\% & Three-block SimpleCNN \\
CNN baseline (40 epochs) & 0.918 & 92.96\% & Three-block SimpleCNN \\
\midrule
\multicolumn{4}{l}{\textit{Deep Learning Models: Hyper-parameter Tuned}} \\[2pt]
\midrule
SimpleCNN (15 epochs) & 0.901 & 91.6\% & Best Hyper-parameters \\
Mobilenet-v3-small & 0.944 & 95.3\% & Best Hyper-parameters \\
EfficientNet-B0 (15 epochs) & 0.982 & 98.5\% & Best Hyper-parameters \\
ResNet-18 (15 epochs) & \textbf{0.985} & \textbf{98.8\%} & Best Hyper-parameters \\
\bottomrule
\end{tabular}
\end{center}
\smallskip
\noindent\textit{Note.} Macro-F1 values for the baseline SimpleCNN runs are computed from the held-out test set using the saved training logs; classical baselines use scikit-learn's classification report.

\section{Analysis}
\textbf{Baselines:} kNN outperforms HOG because colour strongly signals ripeness, whereas HOG discards colour by design.\newline
\textbf{CNN vs baselines:} Learned features that combine colour and texture outperform raw-pixel matching by a large margin; the selected ResNet-18 adds $\approx$24 points of absolute accuracy over kNN.\newline
\textbf{Hyperparameters:} Stronger regularisation (dropout $>\!0.4$) benefits ResNet/EfficientNet; SimpleCNN gains most from moderate colour jitter and restrained dropout.\newline
\textbf{Error patterns:} Confusions are expected between adjacent ripeness stages (e.g., ripe vs. slightly unripe/rotting). We summarise this with the aggregated confusion matrix (Figure~\ref{fig:confusion_matrix_all_hyperparam_runs}); the per-class precision/recall table appears in Appendix~\ref{tab:classification-report}.\\
\textbf{ROC curves:} Figure~\ref{fig:best-roc} overlays per-class ROC curves for all runs, with the selected configuration highlighted. Its near-vertical rise confirms strong performance while the log-scaled FPR axis reveals subtle differences among high-performing runs.\\

Across runs, the largest errors occur for the \emph{unripe} classes across fruits. This aligns with the data (Figure~\ref{fig:dataset-sample-images}): many unripe examples show small fruits on branches with more background and weaker colour cues. Those factors reduce signal-to-noise and make classification harder, explaining the consistent pattern across models.


\begin{figure}[H]
  \centering
  \img[width=0.8\linewidth]{confusion_matrix_all_hyperparam_runs.png}
  \caption{Confusion matrix across all runs in the hyper-parameter tuning.}
  \label{fig:confusion_matrix_all_hyperparam_runs}
\end{figure}



\section{Reproducibility}
See \texttt{README.md} for environment setup and dataset download instructions. Notebooks under \texttt{training notebooks/} reproduce baselines and the CNN experiments;\\ \texttt{training notebooks/cnn/hyperparam\_tuning/} contains the W\&B hyper-parameter sweep setup and analysis. The submission bundle includes step-by-step reproduction instructions, the selected model checkpoint, and the CSV of final test predictions produced by that checkpoint.

\section*{References}
\begin{itemize}
  \item leftin. ``Fruit Ripeness: Unripe, Ripe, and Rotten'' (Kaggle dataset; updated 4 months ago). URL: \href{https://www.kaggle.com/datasets/leftin/fruit-ripeness-unripe-ripe-and-rotten}{https://www.kaggle.com/datasets/leftin/fruit-ripeness-unripe-ripe-and-rotten}
  \item He, K.; Zhang, X.; Ren, S.; Sun, J. ``Deep Residual Learning for Image Recognition.'' arXiv:1512.03385.
  \item Howard, A. et al. ``Searching for MobileNetV3.'' arXiv:1905.02244.
  \item Tan, M.; Le, Q. ``EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.'' arXiv:1905.11946.
\end{itemize}

\section*{Acknowledgements}
We acknowledge use of PyTorch, torchvision, scikit-learn, scikit-image, and Weights and Biases. Baseline methodology and configuration details are documented in the repository under \texttt{training notebooks/}.

\newpage
\appendix
\section{Classification Report (ResNet-18 still-sweep-1)}
\begin{table}[h]
\centering
\caption{Per-class precision/recall/F1 on the test set for the selected ResNet-18 (from W\&B export).}
\label{tab:classification-report}
\begin{tabular}{lccc}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\hline
Fresh Apples & 0.99 & 1.00 & 0.99 \\
Fresh Banana & 0.99 & 0.99 & 0.99 \\
Fresh Oranges & 0.99 & 0.99 & 0.99 \\
Rotten Apples & 0.98 & 0.99 & 0.99 \\
Rotten Banana & 1.00 & 0.98 & 0.99 \\
Rotten Oranges & 0.99 & 0.98 & 0.99 \\
Unripe Apple & 0.97 & 0.95 & 0.96 \\
Unripe Banana & 0.98 & 0.95 & 0.96 \\
Unripe Orange & 0.97 & 0.94 & 0.95 \\
\hline
\textbf{Macro Average} & \textbf{0.99} & \textbf{0.98} & \textbf{0.98} \\
\textbf{Weighted Average} & \textbf{0.99} & \textbf{0.99} & \textbf{0.99} \\
\hline
\end{tabular}
\end{table}

\section{Dataset}
\begin{table}[h]
\centering
\caption{Dataset Split Across 9 Fruit Ripeness Classes}
\label{tab:dataset_split}
\begin{tabular}{lcccc}
\hline
\textbf{Class} & \textbf{Train} & \textbf{Validation} & \textbf{Test} & \textbf{Total} \\
\hline
Fresh Apples & 1,360 & 240 & 340 & 1,940 \\
Fresh Banana & 1,478 & 261 & 390 & 2,129 \\
Fresh Oranges & 1,544 & 273 & 391 & 2,208 \\
Rotten Apples & 1,541 & 272 & 400 & 2,213 \\
Rotten Banana & 1,796 & 317 & 451 & 2,564 \\
Rotten Oranges & 1,595 & 282 & 403 & 2,280 \\
Unripe Apple & 1,464 & 259 & 345 & 2,068 \\
Unripe Banana & 1,545 & 273 & 363 & 2,181 \\
Unripe Orange & 1,461 & 258 & 361 & 2,080 \\
\hline
\textbf{Total} & \textbf{13,784} & \textbf{2,433} & \textbf{3,246} & \textbf{19,463} \\
\hline
\end{tabular}
\end{table}

\end{document}
