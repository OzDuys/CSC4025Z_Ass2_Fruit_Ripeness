{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Fruit Ripeness Classifier\n",
    "\n",
    "Quick-start notebook tuned for Google Colab GPU usage.\n",
    "\n",
    "1. Install dependencies.\n",
    "2. Upload your `kaggle.json` when prompted.\n",
    "3. Download and prepare the dataset (handled below).\n",
    "4. Run the training and evaluation cells.\n",
    "\n",
    "You can also run this locally; skip the Kaggle upload cell if your credentials are already configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "Run this cell once per Colab session to install PyTorch, Kaggle, and analysis libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q kagglehub torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q numpy pandas scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Configure the Runtime\n",
    "Imports the packages needed for data handling, modelling, and plotting, then reports the active device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Kaggle Credentials\n",
    "Upload your Kaggle API token so the dataset can be downloaded automatically. The next cell also fixes the token file permissions required by Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files  # type: ignore\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    files = None\n",
    "    IS_COLAB = False\n",
    "\n",
    "kaggle_dir = Path.home() / \".kaggle\"\n",
    "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "kaggle_json = kaggle_dir / \"kaggle.json\"\n",
    "if not kaggle_json.exists():\n",
    "    if files is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"kaggle.json not found. Upload it in Colab or place it in ~/.kaggle/\"\n",
    "        )\n",
    "    print(\"Upload your kaggle.json file (Account > Create New API Token).\")\n",
    "    uploaded = files.upload()\n",
    "    if not uploaded:\n",
    "        raise ValueError(\"No files uploaded.\")\n",
    "    if \"kaggle.json\" in uploaded:\n",
    "        data = uploaded[\"kaggle.json\"]\n",
    "    else:\n",
    "        filename, data = next(iter(uploaded.items()))\n",
    "        print(f\"Received '{filename}'. Renaming to 'kaggle.json'.\")\n",
    "    kaggle_json.write_bytes(data)\n",
    "    print(\"kaggle.json uploaded.\")\n",
    "else:\n",
    "    print(\"kaggle.json already present; skipping upload.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195afb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287446d0",
   "metadata": {},
   "source": [
    "## 4. Download and Prepare the Dataset\n",
    "Downloads the Kaggle dataset with `kagglehub`, extracts archives, and locates the train/test directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "DATASET_SLUG = \"leftin/fruit-ripeness-unripe-ripe-and-rotten\"\n",
    "TARGET_DIR = Path(\"data/fruit_ripeness_dataset\")\n",
    "FORCE_DOWNLOAD = False  # Set to True to refresh the dataset\n",
    "\n",
    "\n",
    "def iter_files(path: Path):\n",
    "    return [p for p in path.rglob('*') if p.is_file()]\n",
    "\n",
    "\n",
    "def copy_contents(src: Path, dst: Path) -> None:\n",
    "    files = iter_files(src)\n",
    "    if not files:\n",
    "        return\n",
    "    for file_path in tqdm(files, desc=\"Copying dataset files\", unit=\"file\"):\n",
    "        relative = file_path.relative_to(src)\n",
    "        target = dst / relative\n",
    "        target.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(file_path, target)\n",
    "\n",
    "\n",
    "def extract_archives(path: Path) -> None:\n",
    "    zip_files = list(path.rglob(\"*.zip\"))\n",
    "    for zip_path in tqdm(zip_files, desc=\"Extracting archives\", unit=\"zip\"):\n",
    "        extract_dir = zip_path.with_suffix(\"\")\n",
    "        extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            members = zf.namelist()\n",
    "            for member in tqdm(members, desc=f\"Extracting {zip_path.name}\", leave=False, unit=\"file\"):\n",
    "                zf.extract(member, extract_dir)\n",
    "        zip_path.unlink()\n",
    "\n",
    "\n",
    "def find_split_dir(root: Path, name: str):\n",
    "    candidates = sorted(\n",
    "        [p for p in root.rglob(name) if p.is_dir()],\n",
    "        key=lambda p: len(p.parts),\n",
    "    )\n",
    "    for candidate in candidates:\n",
    "        if any(candidate.glob(\"*/*\")):\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "if TARGET_DIR.exists() and not FORCE_DOWNLOAD:\n",
    "    print(f\"Dataset already present at {TARGET_DIR.resolve()}\\nSet FORCE_DOWNLOAD=True to re-download.\")\n",
    "else:\n",
    "    if TARGET_DIR.exists() and FORCE_DOWNLOAD:\n",
    "        shutil.rmtree(TARGET_DIR)\n",
    "    TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Downloading {DATASET_SLUG} with kagglehub ...\")\n",
    "    downloaded_path = Path(kagglehub.dataset_download(DATASET_SLUG)).resolve()\n",
    "    print(f\"Download complete: {downloaded_path}\")\n",
    "    copy_contents(downloaded_path, TARGET_DIR)\n",
    "    extract_archives(TARGET_DIR)\n",
    "    print(f\"Dataset extracted to {TARGET_DIR.resolve()}\")\n",
    "\n",
    "TRAIN_DIR = find_split_dir(TARGET_DIR, \"train\")\n",
    "TEST_DIR = find_split_dir(TARGET_DIR, \"test\")\n",
    "\n",
    "if TRAIN_DIR is None:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not locate a 'train' directory inside {TARGET_DIR.resolve()}\"\n",
    "    )\n",
    "\n",
    "print(f\"Using train directory: {TRAIN_DIR}\")\n",
    "if TEST_DIR is None:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not locate a 'test' directory inside {TARGET_DIR.resolve()}\"\n",
    "    )\n",
    "print(f\"Using test directory: {TEST_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e724d5",
   "metadata": {},
   "source": [
    "## 5. Set Hyperparameters\n",
    "Central place to adjust random seed, batch size, image size, and training duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15  # Used only if no separate test directory exists\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7479366",
   "metadata": {},
   "source": [
    "## 6. Build Datasets and DataLoaders\n",
    "Creates stratified training/validation/test splits, applies transforms, and prepares PyTorch data loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ImageFolder(TRAIN_DIR, transform=None)\n",
    "if len(full_dataset) == 0:\n",
    "    raise RuntimeError(\"ImageFolder found no images in the training directory.\")\n",
    "\n",
    "class_names = full_dataset.classes\n",
    "print(f\"Detected classes: {class_names}\")\n",
    "\n",
    "val_size = max(1, int(len(full_dataset) * VAL_SPLIT))\n",
    "remaining_for_train = len(full_dataset) - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_subset, val_subset = random_split(\n",
    "    full_dataset,\n",
    "    [remaining_for_train, val_size],\n",
    "    generator=generator,\n",
    ")\n",
    "\n",
    "raw_test_dataset = ImageFolder(TEST_DIR, transform=None)\n",
    "if raw_test_dataset.classes != class_names:\n",
    "    raise RuntimeError(\"Class labels differ between train and test directories.\")\n",
    "test_indices = range(len(raw_test_dataset))\n",
    "\n",
    "\n",
    "class SubsetWithTransform(Dataset):\n",
    "    def __init__(self, dataset: ImageFolder, indices, transform):\n",
    "        self.dataset = dataset\n",
    "        self.indices = list(indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[self.indices[idx]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = SubsetWithTransform(full_dataset, train_subset.indices, train_transforms)\n",
    "val_dataset = SubsetWithTransform(full_dataset, val_subset.indices, eval_transforms)\n",
    "test_dataset = SubsetWithTransform(raw_test_dataset, test_indices, eval_transforms)\n",
    "\n",
    "num_workers = 0 if IS_COLAB else min(2, (os.cpu_count() or 1) - 1 if (os.cpu_count() or 1) > 1 else 0)\n",
    "num_workers = max(num_workers, 0)\n",
    "pin_memory = torch.cuda.is_available()\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "print(\n",
    "    f\"Data sizes -> train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define the Baseline CNN\n",
    "A lightweight convolutional network used as the initial benchmark model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * (IMAGE_SIZE // 8) * (IMAGE_SIZE // 8), 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SimpleCNN(num_classes=len(class_names)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Training Utilities\n",
    "Helper functions for training and evaluation, including tqdm progress bars for batch-level insight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch=None, total_epochs=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    desc = \"Training\" if epoch is None or total_epochs is None else f\"Train {epoch:02d}/{total_epochs:02d}\"\n",
    "    progress = tqdm(loader, desc=desc, leave=False, unit=\"batch\")\n",
    "\n",
    "    for images, labels in progress:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        total += images.size(0)\n",
    "\n",
    "        if total:\n",
    "            progress.set_postfix(\n",
    "                loss=running_loss / total,\n",
    "                acc=running_correct / total,\n",
    "            )\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, split=\"Eval\", epoch=None, total_epochs=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if epoch is None or total_epochs is None:\n",
    "        desc = split\n",
    "    else:\n",
    "        desc = f\"{split} {epoch:02d}/{total_epochs:02d}\"\n",
    "    progress = tqdm(loader, desc=desc, leave=False, unit=\"batch\")\n",
    "\n",
    "    for images, labels in progress:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        total += images.size(0)\n",
    "\n",
    "        if total:\n",
    "            progress.set_postfix(\n",
    "                loss=running_loss / total,\n",
    "                acc=running_correct / total,\n",
    "            )\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run the Training Loop\n",
    "Executes the epoch loop with tqdm progress bars to monitor loss and accuracy updates in real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device, epoch=epoch, total_epochs=NUM_EPOCHS\n",
    "    )\n",
    "    val_loss, val_acc = evaluate(\n",
    "        model, val_loader, criterion, device, split=\"Validation\", epoch=epoch, total_epochs=NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{NUM_EPOCHS} | \"\n",
    "        f\"train_loss: {train_loss:.4f}, train_acc: {train_acc:.3f} | \"\n",
    "        f\"val_loss: {val_loss:.4f}, val_acc: {val_acc:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ab19c",
   "metadata": {},
   "source": [
    "## 10. Visualise Training Curves\n",
    "Plots loss and accuracy so you can inspect learning behaviour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, history[\"train_loss\"], label=\"Train\")\n",
    "plt.plot(epochs_range, history[\"val_loss\"], label=\"Validation\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross-entropy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, history[\"train_acc\"], label=\"Train\")\n",
    "plt.plot(epochs_range, history[\"val_acc\"], label=\"Validation\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6944a839",
   "metadata": {},
   "source": [
    "## 11. Evaluate on the Test Set\n",
    "Reports final performance using the held-out split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device, split=\"Test\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b94edf",
   "metadata": {},
   "source": [
    "## 12. (Optional) Save the Trained Model\n",
    "Toggle this to persist the model weights for reuse or submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL = False  # Switch to True to persist the trained weights\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    checkpoint_dir = Path(\"checkpoints\")\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    checkpoint_path = checkpoint_dir / \"baseline_cnn.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"class_names\": class_names,\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Saved checkpoint to {checkpoint_path.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
